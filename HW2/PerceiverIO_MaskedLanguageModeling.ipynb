{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PerceiverIO_MaskedLanguageModeling.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xndAQ2QGJJus"
      },
      "source": [
        "#Samer Baslan\n",
        "#PerceiverIO Homework: Masked Language Modeling\n",
        "#CMPE-297: Special Topics Spring 2021\n",
        "\n",
        "Resource: https://github.com/2796gaurav/code_examples/blob/main/Perceiver/Perceiver_masked_language_modelling.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UBrT4MNVCnR0"
      },
      "source": [
        "# Copyright 2021 DeepMind Technologies Limited\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEz1JBj1C0l0",
        "outputId": "8da2eb43-65f5-4cc8-e396-57840340e86c"
      },
      "source": [
        "#Google Colab dependencies\n",
        "\n",
        "!pip install dm-haiku\n",
        "!pip install einops\n",
        "\n",
        "!mkdir /content/perceiver\n",
        "!touch /content/perceiver/__init__.py\n",
        "!wget -O /content/perceiver/bytes_tokenizer.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/bytes_tokenizer.py\n",
        "!wget -O /content/perceiver/io_processors.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/io_processors.py\n",
        "!wget -O /content/perceiver/perceiver.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/perceiver.py\n",
        "!wget -O /content/perceiver/position_encoding.py https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/position_encoding.py\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (3.7.4.3)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (1.19.5)\n",
            "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.12.0)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/dist-packages (from dm-haiku) (0.8.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.7.1->dm-haiku) (1.15.0)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.7/dist-packages (0.3.2)\n",
            "mkdir: cannot create directory ‘/content/perceiver’: File exists\n",
            "--2021-10-07 05:00:02--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/bytes_tokenizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1850 (1.8K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/bytes_tokenizer.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]   1.81K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-07 05:00:02 (26.6 MB/s) - ‘/content/perceiver/bytes_tokenizer.py’ saved [1850/1850]\n",
            "\n",
            "--2021-10-07 05:00:02--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/io_processors.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 29359 (29K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/io_processors.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]  28.67K  --.-KB/s    in 0.003s  \n",
            "\n",
            "2021-10-07 05:00:03 (9.35 MB/s) - ‘/content/perceiver/io_processors.py’ saved [29359/29359]\n",
            "\n",
            "--2021-10-07 05:00:03--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/perceiver.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 30179 (29K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/perceiver.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]  29.47K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2021-10-07 05:00:03 (19.4 MB/s) - ‘/content/perceiver/perceiver.py’ saved [30179/30179]\n",
            "\n",
            "--2021-10-07 05:00:03--  https://raw.githubusercontent.com/deepmind/deepmind-research/master/perceiver/position_encoding.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8107 (7.9K) [text/plain]\n",
            "Saving to: ‘/content/perceiver/position_encoding.py’\n",
            "\n",
            "/content/perceiver/ 100%[===================>]   7.92K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-10-07 05:00:04 (57.9 MB/s) - ‘/content/perceiver/position_encoding.py’ saved [8107/8107]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9hI6psAUC5sO"
      },
      "source": [
        "from typing import Union\n",
        "\n",
        "import haiku as hk\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "from perceiver import perceiver, position_encoding, io_processors, bytes_tokenizer"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fKKhvi1lDChr"
      },
      "source": [
        "Load parameters from checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3iqkXtLC-hp",
        "outputId": "22971c8e-f37d-4c23-f713-d1663a209830"
      },
      "source": [
        "!wget -O language_perceiver_io_bytes.pickle https://storage.googleapis.com/perceiver_io/language_perceiver_io_bytes.pickle\n",
        "\n",
        "with open(\"language_perceiver_io_bytes.pickle\", \"rb\") as f:\n",
        "  params = pickle.loads(f.read())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-07 05:00:05--  https://storage.googleapis.com/perceiver_io/language_perceiver_io_bytes.pickle\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.157.128, 142.251.8.128, 74.125.203.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.157.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 804479532 (767M) [application/octet-stream]\n",
            "Saving to: ‘language_perceiver_io_bytes.pickle’\n",
            "\n",
            "language_perceiver_ 100%[===================>] 767.21M   144MB/s    in 5.1s    \n",
            "\n",
            "2021-10-07 05:00:10 (149 MB/s) - ‘language_perceiver_io_bytes.pickle’ saved [804479532/804479532]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4I-XIdPDP4o"
      },
      "source": [
        "##Model Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HKF82bCvDHkT"
      },
      "source": [
        "D_MODEL = 768\n",
        "D_LATENTS = 1280\n",
        "MAX_SEQ_LEN = 2048\n",
        "\n",
        "encoder_config = dict(\n",
        "    num_self_attends_per_block = 26,\n",
        "    num_blocks = 1,\n",
        "    z_index_dim = 256,\n",
        "    num_z_channels = D_LATENTS,\n",
        "    num_self_attend_heads = 8,\n",
        "    num_cross_attend_heads = 8,\n",
        "    qk_channels = 8 * 32,\n",
        "    v_channels = D_LATENTS,\n",
        "    use_query_residual = True,\n",
        "    cross_attend_widening_factor = 1,\n",
        "    self_attend_widening_factor = 1\n",
        ")\n",
        "\n",
        "decoder_config = dict(\n",
        "    output_num_channels = D_LATENTS,\n",
        "    position_encoding_type = 'trainable',\n",
        "    output_index_dims = MAX_SEQ_LEN,\n",
        "    num_z_channels = D_LATENTS,\n",
        "    qk_channels = 8 * 32,\n",
        "    v_channels = D_MODEL,\n",
        "    num_heads = 8,\n",
        "    final_project = False,\n",
        "    use_query_residual = False,\n",
        "    trainable_position_encoding_kwargs = dict(num_channels = D_MODEL)\n",
        ")\n",
        "\n",
        "#UTF-8 encoding with an offset\n",
        "tokenizer = bytes_tokenizer.BytesTokenizer()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ho-JipqCEt7Z"
      },
      "source": [
        "##Decoding Perceiver Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pxpN7Q6YEjAc"
      },
      "source": [
        "def apply_perceiver(inputs: jnp.ndarray, input_mask: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"\n",
        "  Runs a forward pass on the Perceiver\n",
        "  Args:\n",
        "    inputs: input bytes, an int array of shape [B, T]\n",
        "    input_mask: Array of shape indicating which entries are valid and which are masked.\n",
        "                A truthy value indicates that the entry is valid\n",
        "  Returns:\n",
        "    The output logits, an array of shape [B, T, vocab_size]\n",
        "  \"\"\"\n",
        "\n",
        "  assert inputs.shape[1] == MAX_SEQ_LEN\n",
        "\n",
        "  embedding_layer = hk.Embed(\n",
        "      vocab_size = tokenizer.vocab_size,\n",
        "      embed_dim = D_MODEL)\n",
        "  embedded_inputs = embedding_layer(inputs)\n",
        "\n",
        "  batch_size = embedded_inputs.shape[0]\n",
        "\n",
        "  input_pos_encoding = perceiver.position_encoding.TrainablePositionEncoding(\n",
        "      index_dim = MAX_SEQ_LEN, num_channels = D_MODEL\n",
        "  )\n",
        "\n",
        "  embedded_inputs = embedded_inputs + input_pos_encoding(batch_size)\n",
        "  perceiver_mod = perceiver.Perceiver(\n",
        "      encoder = perceiver.PerceiverEncoder(**encoder_config),\n",
        "      decoder = perceiver.BasicDecoder(**decoder_config)\n",
        "  )\n",
        "\n",
        "  output_embeddings = perceiver_mod(\n",
        "      embedded_inputs, is_training = False, input_mask = input_mask, query_mask = input_mask\n",
        "  )\n",
        "\n",
        "  logits = io_processors.EmbeddingDecoder(\n",
        "      embedding_matrix = embedding_layer.embeddings)(output_embeddings)\n",
        "  \n",
        "  return logits\n",
        "\n",
        "apply_perceiver = hk.transform(apply_perceiver).apply"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xomeaCARGNP7",
        "outputId": "79da36bc-6159-4759-9d5b-7a7e34903cf7"
      },
      "source": [
        "input_str = \"I'm a big fan of Italian food, especially pizza.\"\n",
        "input_tokens = tokenizer.to_int(input_str)\n",
        "\n",
        "input_tokens[30:41] = tokenizer.mask_token\n",
        "print(\"Tokenized string without masked bytes:\")\n",
        "print(tokenizer.to_string(input_tokens))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized string without masked bytes:\n",
            "I'm a big fan of Italian food, pizza.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bhinw7LtInQ4",
        "outputId": "96894488-cd6e-4c7e-d087-5370cfba3d26"
      },
      "source": [
        "input_str[41:47]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "' pizza'"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S06Mct3JG7J8"
      },
      "source": [
        "##Pad and reshape inputs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFnc5pqiGjOz"
      },
      "source": [
        "inputs = input_tokens[None]\n",
        "input_mask = np.ones_like(inputs)\n",
        "\n",
        "def pad(max_sequence_length: int, inputs, input_mask):\n",
        "  input_len = inputs.shape[1]\n",
        "  assert input_len <= max_sequence_length\n",
        "  pad_len = max_sequence_length - input_len\n",
        "  padded_inputs = np.pad(\n",
        "      inputs,\n",
        "      pad_width = ((0, 0), (0, pad_len)),\n",
        "      constant_values = 0)\n",
        "  padded_mask = np.pad(\n",
        "      input_mask,\n",
        "      pad_width = ((0, 0), (0, pad_len)),\n",
        "      constant_values = 0)\n",
        "  return padded_inputs, padded_mask\n",
        "\n",
        "inputs, input_mask = pad(MAX_SEQ_LEN, inputs, input_mask)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_UvbtchGkv3",
        "outputId": "a157c588-707f-44e2-93a7-941d8dfbd0d5"
      },
      "source": [
        "rng = jax.random.PRNGKey(1)  # Unused\n",
        "\n",
        "out = apply_perceiver(params, rng=rng, inputs=inputs, input_mask=input_mask)\n",
        "\n",
        "masked_tokens_predictions = out[0, 30:41].argmax(axis=-1)\n",
        "print(\"Greedy predictions:\")\n",
        "print(masked_tokens_predictions)\n",
        "print()\n",
        "print(\"Predicted string:\")\n",
        "print(tokenizer.to_string(masked_tokens_predictions))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy predictions:\n",
            "[ 38 107 121 118 107 105 111 103 114 114 127]\n",
            "\n",
            "Predicted string:\n",
            " especially\n"
          ]
        }
      ]
    }
  ]
}