{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceiver_ImageClassification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WfAAfDFb-E-o"
      },
      "source": [
        "#Samer Baslan\n",
        "#Perceiver Image Classification Homework: MNIST dataset\n",
        "#CMPE-297: Special Topics Spring 2021\n",
        "\n",
        "Resource: https://colab.research.google.com/github/keras-team/keras-io/blob/master/examples/vision/ipynb/perceiver_image_classification.ipynb#scrollTo=gZmhMWAPFf5b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1XTOWTTI2Qj"
      },
      "source": [
        "##Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGDszQxrFvAQ",
        "outputId": "a3292cda-da54-426b-a8ef-3cc1f1396c5c"
      },
      "source": [
        "!pip install -U tensorflow-addons"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-addons\n",
            "  Downloading tensorflow_addons-0.14.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 29.2 MB/s eta 0:00:01\r\u001b[K     |▋                               | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 40 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 51 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 61 kB 7.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 7.6 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 81 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 92 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |███                             | 102 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 112 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 122 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 133 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 143 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 153 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████▊                           | 163 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████                           | 174 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 184 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 194 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████                          | 204 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 215 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 225 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 235 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████                         | 245 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 256 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 266 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 276 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 286 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 296 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 307 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 317 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 327 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 337 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 348 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 358 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 368 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 378 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 389 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 399 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 409 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 419 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 430 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 440 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 450 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 460 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 471 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 481 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 491 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 501 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 512 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 522 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 532 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 542 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 552 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 563 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 573 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 583 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 593 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 604 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 614 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 624 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 634 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 645 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 655 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 665 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 675 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 686 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 696 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 706 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 716 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 727 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 737 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 747 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 757 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 768 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 778 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 788 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 798 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 808 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 819 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 829 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 839 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 849 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 860 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 870 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 880 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 890 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 901 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 911 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 921 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 931 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 942 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 952 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 962 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 972 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 983 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 993 kB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.0 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 6.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.1 MB 6.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
            "Installing collected packages: tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.14.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGHP-ROiFvO8"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfKAts4rWWWQ",
        "outputId": "a66d8bff-6d5c-4149-bd72-cd8841f6b412"
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Oct  7 04:15:47 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.74       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   38C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJpdBxEhI4_Q"
      },
      "source": [
        "##Data Prepation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sHhay0UEt-o",
        "outputId": "ea770198-2aa3-49ed-a003-b9a2276f2a16"
      },
      "source": [
        "num_classes = 10\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "y_train = y_train.reshape(y_train.shape[0], 1)\n",
        "y_test = y_test.reshape(y_test.shape[0], 1)\n",
        "\n",
        "\n",
        "print(f\"x_train shape: {x_train.shape} - y_train shape: {y_train.shape}\")\n",
        "print(f\"x_test shape: {x_test.shape} - y_test shape: {y_test.shape}\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n",
            "x_train shape: (60000, 28, 28, 1) - y_train shape: (60000, 1)\n",
            "x_test shape: (10000, 28, 28, 1) - y_test shape: (10000, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOmnUmSrI7Ev"
      },
      "source": [
        "##Hyperparameter Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fSUf_6AoIiEg",
        "outputId": "96846635-6777-4bc8-d330-1cf2b753af93"
      },
      "source": [
        "learning_rate = 0.001\n",
        "weight_decay = 0.0001\n",
        "batch_size = 64\n",
        "num_epochs = 10\n",
        "dropout_rate = 0.2\n",
        "image_size = 56 #Resize input images to this size\n",
        "patch_size = 2 #Size of the patches to be extracted from the input images\n",
        "               #To use each pixel as individual input in the data array, set to 1\n",
        "num_patches = (image_size // patch_size) ** 2 #Size of the data array\n",
        "latent_dim = 256 #Size of latent array\n",
        "projection_dim = 256 #Embedding size of each element in the data and latent array\n",
        "num_heads = 8 #Number of Transformer heads\n",
        "num_transformer_blocks = 4\n",
        "num_iterations = 2 #Repetitions of the cross-attention and Transformer modules\n",
        "\n",
        "ffn_units = [projection_dim, projection_dim,] #Size of the Transformer Feedforward network\n",
        "classifier_units = [\n",
        "                    projection_dim, \n",
        "                    num_classes,\n",
        "                    ] #Size of the Feedforward network of the final classifier\n",
        "\n",
        "print(f\"Image size: {image_size} X {image_size} = {image_size ** 2}\")\n",
        "print(f\"Patch size: {patch_size} X {patch_size} = {patch_size ** 2}\")\n",
        "print(f\"Patches per image: {num_patches}\")\n",
        "print(f\"Elements per patch (3 channels): {(patch_size ** 2) * 3}\")\n",
        "print(f\"Latent array shape: {latent_dim} X {projection_dim}\")\n",
        "print(f\"Data array shape: {num_patches} X {projection_dim}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image size: 56 X 56 = 3136\n",
            "Patch size: 2 X 2 = 4\n",
            "Patches per image: 784\n",
            "Elements per patch (3 channels): 12\n",
            "Latent array shape: 256 X 256\n",
            "Data array shape: 784 X 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gwwjjTMKpVv"
      },
      "source": [
        "##Data Augmentation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-9FjbNqKfCA"
      },
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "     layers.Normalization(),\n",
        "     layers.Resizing(image_size, image_size),\n",
        "     layers.RandomFlip(\"horizontal\"),\n",
        "     layers.RandomZoom(\n",
        "         height_factor = 0.2, width_factor = 0.2\n",
        "     ),\n",
        "    ],\n",
        "    name = \"data_augmentation\",\n",
        ")\n",
        "\n",
        "#Compute the mean and the vavriance of the training data for normalization\n",
        "data_augmentation.layers[0].adapt(x_train)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmMrCT-GLIPe"
      },
      "source": [
        "##Feed Forward Network (FFN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDamtIeFLE7i"
      },
      "source": [
        "def create_ffn(hidden_units, dropout_rate):\n",
        "  ffn_layers = []\n",
        "  for units in hidden_units[:-1]:\n",
        "    ffn_layers.append(layers.Dense(units, activation = tf.nn.gelu))\n",
        "\n",
        "  ffn_layers.append(layers.Dense(units = hidden_units[-1]))\n",
        "  ffn_layers.append(layers.Dropout(dropout_rate))\n",
        "\n",
        "  ffn = keras.Sequential(ffn_layers)\n",
        "  return ffn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnq0VHI-McIh"
      },
      "source": [
        "##Implement patch creation as a layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tr9yQB_AMbH3"
      },
      "source": [
        "class Patches(layers.Layer):\n",
        "  def __init__(self, patch_size):\n",
        "    super(Patches, self).__init__()\n",
        "    self.patch_size = patch_size\n",
        "\n",
        "  def call(self, images):\n",
        "    batch_size = tf.shape(images)[0]\n",
        "    patches = tf.image.extract_patches(\n",
        "        images = images,\n",
        "        sizes = [1, self.patch_size, self.patch_size, 1],\n",
        "        strides = [1, self.patch_size, self.patch_size, 1],\n",
        "        rates = [1, 1, 1, 1],\n",
        "        padding = \"VALID\",\n",
        "    )\n",
        "    patch_dims = patches.shape[-1]\n",
        "    patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
        "    return patches"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZvMpvkDNIpf"
      },
      "source": [
        "##Implement the patch encoding layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sf0f5iIsNtEO"
      },
      "source": [
        "The `PatchEncoder` layer will linearly transform a patch by projecting it into a vector of size `latent_dim`. In addition, it adds a learnable position embedding to the projected vector."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aPmWesJVNCX8"
      },
      "source": [
        "class PatchEncoder(layers.Layer):\n",
        "  def __init__(self, num_patches, projection_dim):\n",
        "    super(PatchEncoder, self).__init__()\n",
        "    self.num_patches = num_patches\n",
        "    self.projection = layers.Dense(units = projection_dim)\n",
        "    self.position_embedding = layers.Embedding(\n",
        "        input_dim = num_patches, output_dim = projection_dim\n",
        "    )\n",
        "\n",
        "  def call(self, patches):\n",
        "    positions = tf.range(start = 0, limit = self.num_patches, delta = 1)\n",
        "    encoded = self.projection(patches) + self.position_embedding(positions)\n",
        "    return encoded"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xFGk_xeSOTp_"
      },
      "source": [
        "##Build the Perceiver model\n",
        "\n",
        "The Perceiver consists of two modules:\n",
        "1. cross-attention module\n",
        "2. standard Transformer with self attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5YpXDISRZNX"
      },
      "source": [
        "###Cross-attention module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQHyH-4EOP-N"
      },
      "source": [
        "def create_cross_attention_module(\n",
        "    latent_dim, data_dim, projection_dim, ffn_units, droput_rate\n",
        "    ):\n",
        "  \n",
        "  inputs = {\n",
        "      #[1, latent_dim, projection_dim]\n",
        "      \"latent_array\": layers.Input(shape = (latent_dim, projection_dim)),\n",
        "      #[batch_size, data_dim, projection_dim] \n",
        "      \"data_array\": layers.Input(shape = (data_dim, projection_dim)), \n",
        "  }\n",
        "\n",
        "  #Apply layer norm to the inputs\n",
        "  latent_array = layers.LayerNormalization(epsilon = 1e-6)(inputs[\"latent_array\"])\n",
        "  data_array = layers.LayerNormalization(epsilon = 1e-6)(inputs[\"data_array\"])\n",
        "\n",
        "  #Create query tensor: [1, latent_dim, projection_dim]\n",
        "  query = layers.Dense(units = projection_dim)(latent_array)\n",
        "\n",
        "  #Create key tensor: [batch_size, data_dim, projection_dim]\n",
        "  key = layers.Dense(units = projection_dim)(data_array)\n",
        "\n",
        "  #Create value tensor: [batch_size, data_dim, projection_dim]\n",
        "  value = layers.Dense(units = projection_dim)(data_array)\n",
        "\n",
        "  #Generate cross-attention outputs: [batch_size, latent_dim, projection_dim]\n",
        "  attention_output = layers.Attention(use_scale = True, dropout = 0.1)(\n",
        "      [query, key, value], return_attention_scores = False\n",
        "  )\n",
        "\n",
        "  #Skip connection to 1\n",
        "  attention_output = layers.Add()([attention_output, latent_array])\n",
        "\n",
        "  #Apply layer norm\n",
        "  attention_output = layers.LayerNormalization(epsilon = 1e-6)(attention_output)\n",
        "\n",
        "  #Apply Feedforward network\n",
        "  ffn = create_ffn(ffn_units, dropout_rate)\n",
        "  outputs = ffn(attention_output)\n",
        "\n",
        "  #Skip connection to 2\n",
        "  outputs = layers.Add()([outputs, attention_output])\n",
        "\n",
        "  #Create the Keras model\n",
        "  model = keras.Model(inputs = inputs, outputs = outputs)\n",
        "  return model"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHn02rxORXXH"
      },
      "source": [
        "###Transformer module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3E0jdE2RLqI"
      },
      "source": [
        "def create_transformer_module(\n",
        "    latent_dim, \n",
        "    projection_dim, \n",
        "    num_heads, \n",
        "    num_transformer_blocks, \n",
        "    ffn_units, \n",
        "    dropout_rate,):\n",
        "\n",
        "  #input shape: [1, latent_dim, projection_dim]\n",
        "  inputs = layers.Input(shape = (latent_dim, projection_dim))\n",
        "\n",
        "  x0 = inputs\n",
        "  #Create multiple layers of the Transformer block\n",
        "  for _ in range(num_transformer_blocks):\n",
        "    #Apply layer normalization 1\n",
        "    x1 = layers.LayerNormalization(epsilon = 1e-6)(x0)\n",
        "    #Create a multi-head self-attention layer\n",
        "    attention_output = layers.MultiHeadAttention(\n",
        "        num_heads = num_heads, key_dim = projection_dim, dropout = 0.1\n",
        "    )(x1, x1)\n",
        "    #Skip connection 1\n",
        "    x2 = layers.Add()([attention_output, x0])\n",
        "    #Apply layer normalization 2\n",
        "    x3 = layers.LayerNormalization(epsilon = 1e-6)(x2)\n",
        "    #Apply feedforward network\n",
        "    ffn = create_ffn(hidden_units = ffn_units, dropout_rate = dropout_rate)\n",
        "    x3 = ffn(x3)\n",
        "    #Skip connection 2\n",
        "    x0 = layers.Add()([x3, x2])\n",
        "\n",
        "  #Create the Keras model\n",
        "  model = keras.Model(inputs = inputs, outputs = x0)\n",
        "  return model"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRnjemebSoGS"
      },
      "source": [
        "###Perceiver model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FLZ_kXdSnDD"
      },
      "source": [
        "class Perceiver(keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size,\n",
        "        data_dim,\n",
        "        latent_dim,\n",
        "        projection_dim,\n",
        "        num_heads,\n",
        "        num_transformer_blocks,\n",
        "        ffn_units,\n",
        "        dropout_rate,\n",
        "        num_iterations,\n",
        "        classifier_units,\n",
        "    ):\n",
        "        super(Perceiver, self).__init__()\n",
        "\n",
        "        self.latent_dim = latent_dim\n",
        "        self.data_dim = data_dim\n",
        "        self.patch_size = patch_size\n",
        "        self.projection_dim = projection_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.num_transformer_blocks = num_transformer_blocks\n",
        "        self.ffn_units = ffn_units\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.classifier_units = classifier_units\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # Create latent array.\n",
        "        self.latent_array = self.add_weight(\n",
        "            shape=(self.latent_dim, self.projection_dim),\n",
        "            initializer=\"random_normal\",\n",
        "            trainable=True,\n",
        "        )\n",
        "\n",
        "        # Create patching module.\n",
        "        self.patcher = Patches(self.patch_size)\n",
        "\n",
        "        # Create patch encoder.\n",
        "        self.patch_encoder = PatchEncoder(self.data_dim, self.projection_dim)\n",
        "\n",
        "        # Create cross-attenion module.\n",
        "        self.cross_attention = create_cross_attention_module(\n",
        "            self.latent_dim,\n",
        "            self.data_dim,\n",
        "            self.projection_dim,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create Transformer module.\n",
        "        self.transformer = create_transformer_module(\n",
        "            self.latent_dim,\n",
        "            self.projection_dim,\n",
        "            self.num_heads,\n",
        "            self.num_transformer_blocks,\n",
        "            self.ffn_units,\n",
        "            self.dropout_rate,\n",
        "        )\n",
        "\n",
        "        # Create global average pooling layer.\n",
        "        self.global_average_pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "        # Create a classification head.\n",
        "        self.classification_head = create_ffn(\n",
        "            hidden_units=self.classifier_units, dropout_rate=self.dropout_rate\n",
        "        )\n",
        "\n",
        "        super(Perceiver, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        # Augment data.\n",
        "        augmented = data_augmentation(inputs)\n",
        "        # Create patches.\n",
        "        patches = self.patcher(augmented)\n",
        "        # Encode patches.\n",
        "        encoded_patches = self.patch_encoder(patches)\n",
        "        # Prepare cross-attention inputs.\n",
        "        cross_attention_inputs = {\n",
        "            \"latent_array\": tf.expand_dims(self.latent_array, 0),\n",
        "            \"data_array\": encoded_patches,\n",
        "        }\n",
        "        # Apply the cross-attention and the Transformer modules iteratively.\n",
        "        for _ in range(self.num_iterations):\n",
        "            # Apply cross-attention from the latent array to the data array.\n",
        "            latent_array = self.cross_attention(cross_attention_inputs)\n",
        "            # Apply self-attention Transformer to the latent array.\n",
        "            latent_array = self.transformer(latent_array)\n",
        "            # Set the latent array of the next iteration.\n",
        "            cross_attention_inputs[\"latent_array\"] = latent_array\n",
        "\n",
        "        # Apply global average pooling to generate a [batch_size, projection_dim] repesentation tensor.\n",
        "        representation = self.global_average_pooling(latent_array)\n",
        "        # Generate logits.\n",
        "        logits = self.classification_head(representation)\n",
        "        return logits\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_xemK-zVAms"
      },
      "source": [
        "##Compile, tran, and evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cp6S5uHBU9HJ"
      },
      "source": [
        "def run_experiment(model):\n",
        "  \n",
        "  #Create LAMB optimizer with weight decay\n",
        "  optimizer = tfa.optimizers.LAMB(\n",
        "      learning_rate = learning_rate, weight_decay_rate = weight_decay,\n",
        "  )\n",
        "\n",
        "  #Compile the model\n",
        "  model.compile(\n",
        "      optimizer = optimizer,\n",
        "      loss = keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "      metrics = [\n",
        "          keras.metrics.SparseCategoricalAccuracy(name = \"acc\"),\n",
        "          keras.metrics.SparseTopKCategoricalAccuracy(5, name = \"top5-acc\"),\n",
        "      ],\n",
        "  )\n",
        "\n",
        "  #Create a learning rate scheduler callback\n",
        "  reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
        "      monitor = \"val_loss\", factor = 0.2, patience = 3\n",
        "  )\n",
        "\n",
        "  #Create an early stopping callback\n",
        "  early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "      monitor = \"vall_loss\", patience = 15, restore_best_weights= True\n",
        "  )\n",
        "\n",
        "  #Fit the model\n",
        "  history = model.fit(\n",
        "      x = x_train,\n",
        "      y = y_train,\n",
        "      batch_size = batch_size,\n",
        "      epochs = num_epochs,\n",
        "      validation_split = 0.1,\n",
        "      callbacks = [early_stopping, reduce_lr],\n",
        "  )\n",
        "\n",
        "  _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
        "  print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
        "  print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
        "\n",
        "\n",
        "  #Return history to plot learning curves\n",
        "  return history"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WYoXHo2V1VG",
        "outputId": "80b67aa3-ec51-42ce-a6b6-8da743e07994"
      },
      "source": [
        "perceiver_classifier = Perceiver(\n",
        "    patch_size,\n",
        "    num_patches,\n",
        "    latent_dim,\n",
        "    projection_dim,\n",
        "    num_heads,\n",
        "    num_transformer_blocks,\n",
        "    ffn_units,\n",
        "    dropout_rate,\n",
        "    num_iterations,\n",
        "    classifier_units,\n",
        ")\n",
        "\n",
        "\n",
        "history = run_experiment(perceiver_classifier)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "844/844 [==============================] - 643s 738ms/step - loss: 1.3782 - acc: 0.5072 - top5-acc: 0.9083 - val_loss: 0.5750 - val_acc: 0.7910 - val_top5-acc: 0.9917\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 2/10\n",
            "844/844 [==============================] - 621s 736ms/step - loss: 0.8768 - acc: 0.6855 - top5-acc: 0.9726 - val_loss: 0.4028 - val_acc: 0.8628 - val_top5-acc: 0.9938\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 3/10\n",
            "844/844 [==============================] - 621s 736ms/step - loss: 0.7308 - acc: 0.7344 - top5-acc: 0.9823 - val_loss: 0.2939 - val_acc: 0.9010 - val_top5-acc: 0.9963\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 4/10\n",
            "844/844 [==============================] - 621s 735ms/step - loss: 0.6435 - acc: 0.7609 - top5-acc: 0.9863 - val_loss: 0.3714 - val_acc: 0.8700 - val_top5-acc: 0.9943\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 5/10\n",
            "844/844 [==============================] - 621s 736ms/step - loss: 0.5545 - acc: 0.7904 - top5-acc: 0.9894 - val_loss: 0.1895 - val_acc: 0.9397 - val_top5-acc: 0.9982\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 6/10\n",
            "844/844 [==============================] - 621s 736ms/step - loss: 0.4894 - acc: 0.8094 - top5-acc: 0.9923 - val_loss: 0.1952 - val_acc: 0.9393 - val_top5-acc: 0.9978\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 7/10\n",
            "844/844 [==============================] - 621s 736ms/step - loss: 0.4498 - acc: 0.8198 - top5-acc: 0.9931 - val_loss: 0.1161 - val_acc: 0.9652 - val_top5-acc: 0.9983\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 8/10\n",
            "844/844 [==============================] - 621s 736ms/step - loss: 0.4168 - acc: 0.8308 - top5-acc: 0.9942 - val_loss: 0.1257 - val_acc: 0.9628 - val_top5-acc: 0.9980\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 9/10\n",
            "844/844 [==============================] - 621s 735ms/step - loss: 0.3984 - acc: 0.8351 - top5-acc: 0.9941 - val_loss: 0.1213 - val_acc: 0.9632 - val_top5-acc: 0.9985\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "Epoch 10/10\n",
            "844/844 [==============================] - 621s 736ms/step - loss: 0.3806 - acc: 0.8407 - top5-acc: 0.9949 - val_loss: 0.1267 - val_acc: 0.9642 - val_top5-acc: 0.9990\n",
            "WARNING:tensorflow:Early stopping conditioned on metric `vall_loss` which is not available. Available metrics are: loss,acc,top5-acc,val_loss,val_acc,val_top5-acc\n",
            "313/313 [==============================] - 42s 134ms/step - loss: 0.1393 - acc: 0.9588 - top5-acc: 0.9985\n",
            "Test accuracy: 95.88%\n",
            "Test top 5 accuracy: 99.85%\n"
          ]
        }
      ]
    }
  ]
}